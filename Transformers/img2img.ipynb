{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text-guided image-to-image generation","metadata":{}},{"cell_type":"markdown","source":"> æœ€åä¸€æ¬¡ä¿®æ”¹: [dingzh@dp.tech](mailto:dingzh@dp.tech)\n>\n> æè¿°: æœ¬æ•™ç¨‹ä¸»è¦å‚è€ƒ [hugging face notebook](https://github.com/huggingface/notebooks/blob/main/diffusers_doc/en/img2img.ipynb)ï¼Œå¯åœ¨ Bohrium Notebook ä¸Šç›´æ¥è¿è¡Œã€‚ä½ å¯ä»¥ç‚¹å‡»ç•Œé¢ä¸Šæ–¹è“è‰²æŒ‰é’® `å¼€å§‹è¿æ¥`ï¼Œé€‰æ‹© `bohrium-notebook:2023-04-07` é•œåƒåŠä»»æ„ä¸€æ¬¾`GPU`èŠ‚ç‚¹é…ç½®ï¼Œç¨ç­‰ç‰‡åˆ»å³å¯è¿è¡Œã€‚\n> å¦‚æ‚¨é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·è”ç³» [bohrium@dp.tech](mailto:bohrium@dp.tech) ã€‚\n>\n> å…±äº«åè®®: æœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](https://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚\n\nThis notebook shows how to fine-tune any pretrained Vision model for Image Classification on a custom dataset. The idea is to add a randomly initialized classification head on top of a pre-trained encoder, and fine-tune the model altogether on a labeled dataset.","metadata":{}},{"cell_type":"markdown","source":"The [StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline) lets you pass a text prompt and an initial image to condition the generation of new images.\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\n!pip install diffusers transformers ftfy accelerate\n```\n\nGet started by creating a [StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline) with a pretrained Stable Diffusion model like [`nitrosocke/Ghibli-Diffusion`](https://huggingface.co/nitrosocke/Ghibli-Diffusion).","metadata":{}},{"cell_type":"code","source":"pip install diffusers transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nos.environ['HTTP_PROXY'] = 'http://ga.dp.tech:8118'\nos.environ['HTTPS_PROXY'] = 'http://ga.dp.tech:8118'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\ndevice = \"cuda\"\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"nitrosocke/Ghibli-Diffusion\", torch_dtype=torch.float16).to(\n    device\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download and preprocess an initial image so you can pass it to the pipeline:","metadata":{}},{"cell_type":"code","source":"url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image.thumbnail((768, 768))\ninit_image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/image_2_image_using_diffusers_cell_8_output_0.jpeg\"/>\n</div>\n\n<Tip>\n\nğŸ’¡ `strength` is a value between 0.0 and 1.0 that controls the amount of noise added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input.\n\n</Tip>\n\nDefine the prompt (for this checkpoint finetuned on Ghibli-style art, you need to prefix the prompt with the `ghibli style` tokens) and run the pipeline:","metadata":{}},{"cell_type":"code","source":"prompt = \"ghibli style, a fantasy landscape with castles\"\ngenerator = torch.Generator(device=device).manual_seed(1024)\nimage = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5, generator=generator).images[0]\nimage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ghibli-castles.png\"/>\n</div>\n\nYou can also try experimenting with a different scheduler to see how that affects the output:","metadata":{}},{"cell_type":"code","source":"from diffusers import LMSDiscreteScheduler\n\nlms = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\npipe.scheduler = lms\ngenerator = torch.Generator(device=device).manual_seed(1024)\nimage = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5, generator=generator).images[0]\nimage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lms-ghibli.png\"/>\n</div>\n\nCheck out the Spaces below, and try generating images with different values for `strength`. You'll notice that using lower values for `strength` produces images that are more similar to the original image.\n\nFeel free to also switch the scheduler to the [LMSDiscreteScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler) and see how that affects the output.\n\n<iframe\n\tsrc=\"https://stevhliu-ghibli-img2img.hf.space\"\n\tframeborder=\"0\"\n\twidth=\"850\"\n\theight=\"500\"\n></iframe>","metadata":{}}]}
{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§¨ Stable Diffusion in JAX / Flax !","metadata":{}},{"cell_type":"markdown","source":"> æœ€åŽä¸€æ¬¡ä¿®æ”¹: [dingzh@dp.tech](mailto:dingzh@dp.tech)\n>\n> æè¿°: æœ¬æ•™ç¨‹ä¸»è¦å‚è€ƒ [hugging face notebook](https://github.com/huggingface/notebooks/blob/main/diffusers_doc/en/stable_diffusion_jax_how_to.ipynb)ï¼Œå¯åœ¨ Bohrium Notebook ä¸Šç›´æŽ¥è¿è¡Œã€‚ä½ å¯ä»¥ç‚¹å‡»ç•Œé¢ä¸Šæ–¹è“è‰²æŒ‰é’® `å¼€å§‹è¿žæŽ¥`ï¼Œé€‰æ‹© `bohrium-notebook:2023-04-07` é•œåƒåŠä»»æ„ä¸€æ¬¾`GPU`èŠ‚ç‚¹é…ç½®ï¼Œç¨ç­‰ç‰‡åˆ»å³å¯è¿è¡Œã€‚\n> å¦‚æ‚¨é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·è”ç³» [bohrium@dp.tech](mailto:bohrium@dp.tech) ã€‚\n>\n> å…±äº«åè®®: æœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éžå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](https://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚","metadata":{}},{"cell_type":"markdown","source":"ðŸ¤— Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform.\n\nThis notebook shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this notebook](https://huggingface.co/docs/diffusers/stable_diffusion).\n\nFirst, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \"Change runtime type\" and then select `TPU` under the `Hardware accelerator` setting.\n\nNote that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"markdown","source":"First make sure diffusers is installed.\n\n```bash\n!pip install jax==0.3.25 jaxlib==0.3.25 flax transformers ftfy\n!pip install diffusers\n```","metadata":{}},{"cell_type":"code","source":"pip install jax==0.3.25 jaxlib==0.3.25 flax transformers ftfy diffusers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.tools.colab_tpu\n\n# jax.tools.colab_tpu.setup_tpu()\nimport jax","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_devices = jax.device_count()\n# device_type = jax.devices()[0].device_kind\n\n# print(f\"Found {num_devices} JAX devices of type {device_type}.\")\n# assert (\n#     \"TPU\" in device_type\n# ), \"Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```python out\nFound 8 JAX devices of type Cloud TPU.\n```\n\nThen we import all the dependencies.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport jax\nimport jax.numpy as jnp\n\nfrom pathlib import Path\nfrom jax import pmap\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom PIL import Image\n\nfrom huggingface_hub import notebook_login\nfrom diffusers import FlaxStableDiffusionPipeline","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Loading","metadata":{}},{"cell_type":"markdown","source":"TPU devices support `bfloat16`, an efficient half-float type. We'll use it for our tests, but you can also use `float32` to use full precision instead.","metadata":{}},{"cell_type":"code","source":"dtype = jnp.bfloat16","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Flax is a functional framework, so models are stateless and parameters are stored outside them. Loading the pre-trained Flax pipeline will return both the pipeline itself and the model weights (or parameters). We are using a `bf16` version of the weights, which leads to type warnings that you can safely ignore.","metadata":{}},{"cell_type":"code","source":"pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    revision=\"bf16\",\n    dtype=dtype,\n)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"markdown","source":"Since TPUs usually have 8 devices working in parallel, we'll replicate our prompt as many times as devices we have. Then we'll perform inference on the 8 devices at once, each responsible for generating one image. Thus, we'll get 8 images in the same amount of time it takes for one chip to generate a single one.\n\nAfter replicating the prompt, we obtain the tokenized text ids by invoking the `prepare_inputs` function of the pipeline. The length of the tokenized text is set to 77 tokens, as required by the configuration of the underlying CLIP Text model.","metadata":{}},{"cell_type":"code","source":"prompt = \"A cinematic film still of Morgan Freeman starring as Jimi Hendrix, portrait, 40mm lens, shallow depth of field, close up, split lighting, cinematic\"\nprompt = [prompt] * jax.device_count()\nprompt_ids = pipeline.prepare_inputs(prompt)\nprompt_ids.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```python out\n(8, 77)\n```","metadata":{}},{"cell_type":"markdown","source":"### Replication and parallelization","metadata":{}},{"cell_type":"markdown","source":"Model parameters and inputs have to be replicated across the 8 parallel devices we have. The parameters dictionary is replicated using `flax.jax_utils.replicate`, which traverses the dictionary and changes the shape of the weights so they are repeated 8 times. Arrays are replicated using `shard`.","metadata":{}},{"cell_type":"code","source":"p_params = replicate(params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_ids = shard(prompt_ids)\nprompt_ids.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```python out\n(8, 1, 77)\n```\n\nThat shape means that each one of the `8` devices will receive as an input a `jnp` array with shape `(1, 77)`. `1` is therefore the batch size per device. In TPUs with sufficient memory, it could be larger than `1` if we wanted to generate multiple images (per chip) at once.\n\nWe are almost ready to generate images! We just need to create a random number generator to pass to the generation function. This is the standard procedure in Flax, which is very serious and opinionated about random numbers â€“ all functions that deal with random numbers are expected to receive a generator. This ensures reproducibility, even when we are training across multiple distributed devices.\n\nThe helper function below uses a seed to initialize a random number generator. As long as we use the same seed, we'll get the exact same results. Feel free to use different seeds when exploring results later in the notebook.","metadata":{}},{"cell_type":"code","source":"def create_key(seed=0):\n    return jax.random.PRNGKey(seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We obtain a rng and then \"split\" it 8 times so each device receives a different generator. Therefore, each device will create a different image, and the full process is reproducible.","metadata":{}},{"cell_type":"code","source":"rng = create_key(0)\nrng = jax.random.split(rng, jax.device_count())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"JAX code can be compiled to an efficient representation that runs very fast. However, we need to ensure that all inputs have the same shape in subsequent calls; otherwise, JAX will have to recompile the code, and we wouldn't be able to take advantage of the optimized speed.\n\nThe Flax pipeline can compile the code for us if we pass `jit = True` as an argument. It will also ensure that the model runs in parallel in the 8 available devices.\n\nThe first time we run the following cell it will take a long time to compile, but subequent calls (even with different inputs) will be much faster. For example, it took more than a minute to compile in a TPU v2-8 when I tested, but then it takes about **`7s`** for future inference runs.\n\n```\n%%time\nimages = pipeline(prompt_ids, p_params, rng, jit=True)[0]\n```\n\n```python out\nCPU times: user 56.2 s, sys: 42.5 s, total: 1min 38s\nWall time: 1min 29s\n```\n\nThe returned array has shape `(8, 1, 512, 512, 3)`. We reshape it to get rid of the second dimension and obtain 8 images of `512 Ã— 512 Ã— 3` and then convert them to PIL.","metadata":{}},{"cell_type":"code","source":"images = pipeline(prompt_ids, p_params, rng, jit=True)[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = images.reshape((images.shape[0] * images.shape[1],) + images.shape[-3:])\nimages = pipeline.numpy_to_pil(images)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"markdown","source":"Let's create a helper function to display images in a grid.","metadata":{}},{"cell_type":"code","source":"def image_grid(imgs, rows, cols):\n    w, h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i % cols * w, i // cols * h))\n    return grid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_grid(images, 2, 4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![img](https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/stable_diffusion_jax_how_to_cell_38_output_0.jpeg)","metadata":{}},{"cell_type":"markdown","source":"## Using different prompts","metadata":{}},{"cell_type":"markdown","source":"We don't have to replicate the _same_ prompt in all the devices. We can do whatever we want: generate 2 prompts 4 times each, or even generate 8 different prompts at once. Let's do that!\n\nFirst, we'll refactor the input preparation code into a handy function:","metadata":{}},{"cell_type":"code","source":"prompts = [\n    \"Labrador in the style of Hokusai\",\n    \"Painting of a squirrel skating in New York\",\n    \"HAL-9000 in the style of Van Gogh\",\n    \"Times Square under water, with fish and a dolphin swimming around\",\n    \"Ancient Roman fresco showing a man working on his laptop\",\n    \"Close-up photograph of young black woman against urban background, high quality, bokeh\",\n    \"Armchair in the shape of an avocado\",\n    \"Clown astronaut in space, with Earth in the background\",\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_ids = pipeline.prepare_inputs(prompts)\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, p_params, rng, jit=True).images\nimages = images.reshape((images.shape[0] * images.shape[1],) + images.shape[-3:])\nimages = pipeline.numpy_to_pil(images)\n\nimage_grid(images, 2, 4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![img](https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/stable_diffusion_jax_how_to_cell_43_output_0.jpeg)","metadata":{}},{"cell_type":"markdown","source":"## How does parallelization work?","metadata":{}},{"cell_type":"markdown","source":"We said before that the `diffusers` Flax pipeline automatically compiles the model and runs it in parallel on all available devices. We'll now briefly look inside that process to show how it works.\n\nJAX parallelization can be done in multiple ways. The easiest one revolves around using the `jax.pmap` function to achieve single-program, multiple-data (SPMD) parallelization. It means we'll run several copies of the same code, each on different data inputs. More sophisticated approaches are possible, we invite you to go over the [JAX documentation](https://jax.readthedocs.io/en/latest/index.html) and the [`pjit` pages](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html?highlight=pjit) to explore this topic if you are interested!\n\n`jax.pmap` does two things for us:\n- Compiles (or `jit`s) the code, as if we had invoked `jax.jit()`. This does not happen when we call `pmap`, but the first time the pmapped function is invoked.\n- Ensures the compiled code runs in parallel in all the available devices.\n\nTo show how it works we `pmap` the `_generate` method of the pipeline, which is the private method that runs generates images. Please, note that this method may be renamed or removed in future releases of `diffusers`.","metadata":{}},{"cell_type":"code","source":"p_generate = pmap(pipeline._generate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After we use `pmap`, the prepared function `p_generate` will conceptually do the following:\n* Invoke a copy of the underlying function `pipeline._generate` in each device.\n* Send each device a different portion of the input arguments. That's what sharding is used for. In our case, `prompt_ids` has shape `(8, 1, 77, 768)`. This array will be split in `8` and each copy of `_generate` will receive an input with shape `(1, 77, 768)`.\n\nWe can code `_generate` completely ignoring the fact that it will be invoked in parallel. We just care about our batch size (`1` in this example) and the dimensions that make sense for our code, and don't have to change anything to make it work in parallel.\n\nThe same way as when we used the pipeline call, the first time we run the following cell it will take a while, but then it will be much faster.\n\n```\n%%time\nimages = p_generate(prompt_ids, p_params, rng)\nimages = images.block_until_ready()\nimages.shape\n```\n\n```python out\nCPU times: user 1min 15s, sys: 18.2 s, total: 1min 34s\nWall time: 1min 15s\n```","metadata":{}},{"cell_type":"code","source":"images.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```python out\n(8, 1, 512, 512, 3)\n```\n\nWe use `block_until_ready()` to correctly measure inference time, because JAX uses asynchronous dispatch and returns control to the Python loop as soon as it can. You don't need to use that in your code; blocking will occur automatically when you want to use the result of a computation that has not yet been materialized.","metadata":{}}]}